# Project name for Docker Compose
name: hf4-v1

services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: hf4-v1-postgres
    restart: unless-stopped

    # Port mapping
    ports:
      - "${POSTGRES_PORT:-5432}:5432"

    # Environment variables
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-rag_assistant}
      POSTGRES_USER: ${POSTGRES_USER:-rag_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      # PostgreSQL configuration optimized for vector operations
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=en_US.UTF-8"

    # Volume mapping for data persistence
    volumes:
      - ./postgres-data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
      - ./database/config.sql:/docker-entrypoint-initdb.d/02-config.sql:ro

    # Health check to ensure database is ready
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-rag_user} -d ${POSTGRES_DB:-rag_assistant}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

    # Resource limits (adjust based on your needs)
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

    # Custom PostgreSQL configuration
    command: >
      postgres
      -c shared_buffers=1GB
      -c effective_cache_size=3GB
      -c maintenance_work_mem=512MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=32MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_worker_processes=4
      -c max_parallel_workers_per_gather=2
      -c max_parallel_workers=4
      -c max_parallel_maintenance_workers=2

    networks:
      - hf4-v1-network

  # Optional: PgAdmin for database management
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: hf4-v1-pgadmin
    restart: unless-stopped
    profiles:
      - admin

    ports:
      - "${PGADMIN_PORT:-5050}:80"

    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@example.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}
      PGADMIN_CONFIG_SERVER_MODE: 'False'

    volumes:
      - ./pgadmin-data:/var/lib/pgadmin

    depends_on:
      postgres:
        condition: service_healthy

    networks:
      - hf4-v1-network

  # ============================================================================
  # OBSERVABILITY STACK - OpenTelemetry + Jaeger + Prometheus + Grafana
  # ============================================================================

  # OpenTelemetry Collector - Telemetry aggregation and routing
  # Receives OTLP telemetry from Next.js app and routes to backends
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: hf4-v1-otel-collector
    restart: unless-stopped

    ports:
      - "4317:4317"     # OTLP gRPC receiver
      - "4318:4318"     # OTLP HTTP receiver
      - "8889:8889"     # Prometheus exporter

    volumes:
      - ./otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml:ro

    command: ["--config=/etc/otelcol-contrib/config.yaml"]

    depends_on:
      jaeger:
        condition: service_healthy

    networks:
      - hf4-v1-network

  # Jaeger All-in-One - Distributed tracing backend
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: hf4-v1-jaeger
    restart: unless-stopped

    ports:
      - "16686:16686"   # Jaeger UI
      - "14250:14250"   # Jaeger gRPC (for OTel Collector)

    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key

    volumes:
      - ./jaeger-data:/badger

    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:16686"]
      interval: 10s
      timeout: 5s
      retries: 3

    networks:
      - hf4-v1-network

  # Prometheus - Metrics storage and querying
  prometheus:
    image: prom/prometheus:v2.48.1
    container_name: hf4-v1-prometheus
    restart: unless-stopped

    ports:
      - "9090:9090"     # Prometheus UI

    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus-data:/prometheus

    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'

    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3

    depends_on:
      - otel-collector

    networks:
      - hf4-v1-network

  # Grafana - Metrics visualization and dashboarding
  grafana:
    image: grafana/grafana:10.2.3
    container_name: hf4-v1-grafana
    restart: unless-stopped

    ports:
      - "3001:3000"     # Grafana UI (port 3001 to avoid conflict with Next.js)

    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel

    volumes:
      - ./grafana-data:/var/lib/grafana

    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3

    depends_on:
      prometheus:
        condition: service_healthy

    networks:
      - hf4-v1-network

networks:
  hf4-v1-network:
    driver: bridge

volumes:
  postgres-data:
  pgadmin-data:
  jaeger-data:
  prometheus-data:
  grafana-data:
